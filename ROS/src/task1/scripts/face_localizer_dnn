#!/usr/bin/python3

import sys
import rospy
import cv2
import numpy as np
import tf2_geometry_msgs
import tf2_ros
import os

from os.path import dirname, join

import matplotlib.pyplot as plt
from sensor_msgs.msg import Image
from geometry_msgs.msg import PointStamped, Vector3, Pose
from cv_bridge import CvBridge, CvBridgeError
from visualization_msgs.msg import Marker, MarkerArray
from std_msgs.msg import Bool, ColorRGBA

from geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped
from actionlib_msgs.msg import GoalID, GoalStatusArray

from sound_play.msg import SoundRequest
from sound_play.libsoundplay import SoundClient

from sensor_msgs.msg import CameraInfo
from image_geometry import PinholeCameraModel

import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image as _i

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

STATUS_DICT = {
    0:"The goal has yet to be processed by the action server",
    1:"The goal is currently being processed by the action server",
    2:"The goal received a cancel request after it started executing and has since completed its execution (Terminal State)",
    3:"The goal was achieved successfully by the action server (Terminal State)",
    4:"The goal was aborted during execution by the action server due to some failure (Terminal State)",
    5:"The goal was rejected by the action server without being processed, because the goal was unattainable or invalid (Terminal State)",
    6:"The goal received a cancel request after it started executing and has not yet completed execution",
    7:"The goal received a cancel request before it started executing but the action server has not yet confirmed that the goal is canceled",
    8:"The goal received a cancel request before it started executing and was successfully cancelled (Terminal State)",
    9:"An action client can determine that a goal is LOST. This should not be sent over the wire by an action server",
}

def speak_to_person():
    soundhandle = SoundClient()
    rospy.sleep(1)

    voice = "voice_kal_diphone"
    volume = 1.0
    text = "Hello, I am your robot friend"
    # rospy.sleep(1)
    rospy.loginfo(f"volume: {volume}, voice: {voice}, text:{text}")
    soundhandle.say(text, voice, volume)
    # rospy.sleep(1)


def read_path_log_orientation(filename):
    path = []
    with open(filename, "r") as f:
        for line in f:
            
            if line.strip().startswith("position:"):
                
                x = float(f.readline().strip().split(" ")[1])
                y = float(f.readline().strip().split(" ")[1])
                z = float(f.readline().strip().split(" ")[1])
                # skipings text "orinetation: "
                f.readline()
                x_orientation = float(f.readline().strip().split(" ")[1])
                y_orientation = float(f.readline().strip().split(" ")[1])
                z_orientation = float(f.readline().strip().split(" ")[1])
                w_orientation = float(f.readline().strip().split(" ")[1])

                path.append((x, y, z, z_orientation, w_orientation))
                # path.append(p)
    print(path)
    return path

def read_path_log(filename):
    path = []
    with open(filename, "r") as f:
        for line in f:
            if line.strip().startswith("x: "):
                x = float(line.strip().split(" ")[1])
                y = float(f.readline().strip().split(" ")[1])
                z = float(f.readline().strip().split(" ")[1])
                path.append((x, y, z))
    return path
class face_localizer:

    def __init__(self):
        rospy.init_node('face_localizer', anonymous=True)

        # An object we use for converting images between ROS format and OpenCV format
        self.bridge = CvBridge()
        
        # The function for performin HOG face detection
        #self.face_detector = dlib.get_frontal_face_detector()
        protoPath = join(dirname(__file__), "deploy.prototxt.txt")
        modelPath = join(dirname(__file__), "res10_300x300_ssd_iter_140000.caffemodel")

        self.face_net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)

        # A help variable for holding the dimensions of the image
        self.dims = (0, 0, 0)

        # Marker array object used for showing markers in Rviz
        self.marker_array = MarkerArray()
        self.marker_num = 1

        # Subscribe to the image and/or depth topic
        # self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)
        # self.depth_sub = rospy.Subscriber("/camera/depth/image_raw", Image, self.depth_callback)

        # Publiser for the visualization markers
        self.markers_pub = rospy.Publisher('face_markers', MarkerArray, queue_size=1000)

        # Object we use for transforming between coordinate frames
        self.tf_buf = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buf)
        
        self.simple_goal_pub = rospy.Publisher("/move_base_simple/goal", PoseStamped, queue_size=10)
        self.cancel_goal_pub = rospy.Publisher("/move_base/cancel", GoalID, queue_size=10)

        # resnet18 should suffice, all possible modes: 18, 34, 50, 101, 152
        # COMMENT: resnet18 doesn't suffice, all faces are pretty similar then
        # COMMENT: resnet101 works beautifully, but is quite slow at the end
        self.model = models.resnet101(pretrained=True)
        self.model.to(DEVICE)
        self.model.eval()
        for param in self.model.parameters():
            param.requires_grad = False
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        # full path of the robot
        # points_path = os.path.join(os.path.dirname(__file__), "newpoints.txt")
        # self.path = read_path_log_orientation(points_path)

        # points_path = os.path.join(os.path.dirname(__file__), "points_full_small")
        points_path = os.path.join(os.path.dirname(__file__), "points_full")
        self.path = read_path_log(points_path)

        self.face_path = []
        
        self.path_idx = 0
        self.face_path_idx = 0
        
        self.go_to_face = False

        self.number_of_faces_in_current_movement = 0

        self.all_goals_reached = False
        self.goal_reached = False

        if self.path:
            self.next_goal = True

        self.face_search = True
        # array containing face positions
        self.face_positions_and_images = []
        self.FACE_DISTANCE_TOLERANCE = 1

        # array of robot positions when faces are detected
        self.bot_positions = []
   
    def get_greeting_pose(self, coords, dist, stamp, pose_of_detection: PoseWithCovarianceStamped) -> Pose:
    # def get_pose(self,coords,dist,stamp):
        # Calculate the position of the detected face
        k_f = 554 # kinect focal length in pixels

        x1, x2, y1, y2 = coords

        face_x = self.dims[1] / 2 - (x1+x2)/2.
        face_y = self.dims[0] / 2 - (y1+y2)/2.

        angle_to_target = np.arctan2(face_x,k_f)

        # Get the angles in the base_link relative coordinate system
        # x, y = 0.8 * dist*np.cos(angle_to_target), dist*np.sin(angle_to_target)
        # step away from image 0.4 units
        dist -= 0.4
        x, y = dist*np.cos(angle_to_target), dist*np.sin(angle_to_target)

        ### Define a stamped message for transformation - directly in "base_link"
        #point_s = PointStamped()
        #point_s.point.x = x
        #point_s.point.y = y
        #point_s.point.z = 0.3
        #point_s.header.frame_id = "base_link"
        #point_s.header.stamp = rospy.Time(0)

        # Define a stamped message for transformation - in the "camera rgb frame"
        point_s = PointStamped()
        point_s.point.x = -y
        point_s.point.y = 0
        point_s.point.z = x
        point_s.header.frame_id = "camera_rgb_optical_frame"
        point_s.header.stamp = stamp

        # Get the point in the "map" coordinate system
        try:
            point_world = self.tf_buf.transform(point_s, "map")

            # Create a Pose object with the same position
            pose = Pose()
            pose.position.x = point_world.point.x
            pose.position.y = point_world.point.y
            pose.position.z = point_world.point.z
            bot_relative_orientation = angle_to_target/np.pi
            pose_orientation = pose_of_detection.pose.pose.orientation.z + bot_relative_orientation
            if pose_orientation > 1:
                pose_orientation -= 2
            elif pose_orientation < -1:
                pose_orientation += 2
            pose.orientation.z = pose_orientation
        except Exception as e:
            print(e)
            pose = None

        return pose

    # def status_reached(self) -> tuple[bool, int]:
    def status_reached(self):
        """
        Function listenes for status updates on topic /move_base/status.

        If status is 3 (goal reached) or 4 (goal terminated), the goal is 'reached'.

        TODO: change desc
        """
        status = rospy.wait_for_message("/move_base/status", GoalStatusArray)
        if status.status_list[-1].status in (0, 1):
            # goal is being processed
            return False,status.status_list[-1].status
        else:
            # goal is done (successfully or not)
            return True,status.status_list[-1].status
    
    def publish_new_position(self, log:bool=True) -> None:
        """
        Publishes a new position to the robot.

        If go_to_face, go to the 'position_idx' of the self.face_path, else go to 'position_idx' of self.path.
        """
        msg = PoseStamped()
        msg.header.frame_id = "map"
        msg.header.stamp = rospy.Time().now()
        # TODO: add orientation
        if self.go_to_face:
            # msg.pose = self.face_path[position_idx]
            position_idx = self.face_path_idx
            if position_idx >= len(self.face_path):
                return
            msg.pose.position.x = self.face_path[position_idx][0]
            msg.pose.position.y = self.face_path[position_idx][1]
            msg.pose.orientation.z = self.face_path[position_idx][3]
            msg.pose.orientation.w = 1
            self.face_path_idx += 1
        else:
            # msg.pose = self.path[position_idx]
            position_idx = self.path_idx
            if position_idx >= len(self.path):
                return
            msg.pose.orientation.w = 1
            msg.pose.position.x = self.path[position_idx][0]
            msg.pose.position.y = self.path[position_idx][1]
            # msg.pose.orientation.z = self.path[position_idx][3]
            # msg.pose.orientation.w = self.path[position_idx][4]
            self.path_idx += 1
        if log:
            self.simple_goal_pub.publish(msg)
            array = "face_path" if self.go_to_face else "path"
            rospy.loginfo(f"Published goal at {position_idx} for array {array}.")
            rospy.loginfo(msg)


    def check_status(self, data: GoalStatusArray):
        slist = data.status_list
        if len(slist) == 0:
            self.next_goal = True
            return
        # rospy.loginfo(f"status {slist[-1].status}: {STATUS_DICT[slist[-1].status]}")
        # rospy.loginfo(f"currently_on_path: {self.path}")
        if slist[-1].status == 3:
            if not self.path:
                self.all_goals_reached = True
                rospy.loginfo("All goals reached.")
            self.next_goal = True
        elif slist[-1].status not in (0, 1,):
            self.next_goal = True


    def face_similarity(self, pose, face_region: np.ndarray):
        """
        Calculates similarities between faces. 

        If the detected face is not near any other faces, it should get added.
        If the detected face is near other faces, nn should be run to determine if it's the same face or not.
        """
        # Calculate the distance to the marker
        
        min_dist = float("inf")
        min_face_region = None
        for n, (position, curr_face_region) in enumerate(self.face_positions_and_images):
            dist = np.sqrt((pose.position.x - position[0])**2 + (pose.position.y - position[1])**2)
            if dist < min_dist:
                min_dist = dist
                min_face_region = curr_face_region
                min_idx = n

        if min_dist == float("inf") and self.face_positions_and_images:
            return 0
        
        similarity = 0
        if min_dist < self.FACE_DISTANCE_TOLERANCE and min_face_region is not None:

            face_region = _i.fromarray(face_region)
            min_face_region = _i.fromarray(min_face_region)

            img1 = self.transform(face_region).unsqueeze(0)
            img2 = self.transform(min_face_region).unsqueeze(0)

            with torch.no_grad():
               feature1 = self.model(img1)
               feature2 = self.model(img2)

            similarity = torch.nn.functional.cosine_similarity(feature1, feature2)
             
            print(f"similarity score between new face and face_{min_idx}.jpg : {similarity}")

            # this similarity score was set arbitrarily, but it seems to work just fine
            if similarity < 0.5:
               return self.FACE_DISTANCE_TOLERANCE + 1

        
        return min_dist

    def get_pose(self,coords,dist,stamp):
        # Calculate the position of the detected face

        k_f = 554 # kinect focal length in pixels

        x1, x2, y1, y2 = coords

        face_x = self.dims[1] / 2 - (x1+x2)/2.
        face_y = self.dims[0] / 2 - (y1+y2)/2.

        angle_to_target = np.arctan2(face_x,k_f)

        # Get the angles in the base_link relative coordinate system
        x, y = dist*np.cos(angle_to_target), dist*np.sin(angle_to_target)

        ### Define a stamped message for transformation - directly in "base_link"
        #point_s = PointStamped()
        #point_s.point.x = x
        #point_s.point.y = y
        #point_s.point.z = 0.3
        #point_s.header.frame_id = "base_link"
        #point_s.header.stamp = rospy.Time(0)

        # Define a stamped message for transformation - in the "camera rgb frame"
        point_s = PointStamped()
        point_s.point.x = -y
        point_s.point.y = 0
        point_s.point.z = x
        point_s.header.frame_id = "camera_rgb_optical_frame"
        point_s.header.stamp = stamp

        # Get the point in the "map" coordinate system
        try:
            point_world = self.tf_buf.transform(point_s, "map")

            # Create a Pose object with the same position
            pose = Pose()
            pose.position.x = point_world.point.x
            pose.position.y = point_world.point.y
            pose.position.z = point_world.point.z
        except Exception as e:
            print(e)
            pose = None

        return pose
    

    def aproach_face_in_view(self):
        """
        Aproach the face in view
        This function will be called when the robot is aproaching near a face and then it will wait for robot to print status 3 (finnished)
        after that it will get closer to the fece and it will try to center it in the camera view
        """
        while True:
            #just wait
            status = rgb_image_message = rospy.wait_for_message("/move_base/status", GoalStatusArray)
            if status.status_list[-1].status == 3:
                break
            pass
        # rospy.loginfo("Aproaching face in view")

                # Get the next rgb and depth images that are posted from the camera
        try:
            rgb_image_message = rospy.wait_for_message("/camera/rgb/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        try:
            depth_image_message = rospy.wait_for_message("/camera/depth/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        # Convert the images into a OpenCV (numpy) format

        try:
            rgb_image = self.bridge.imgmsg_to_cv2(rgb_image_message, "bgr8")
        except CvBridgeError as e:
            print(e)

        try:
            depth_image = self.bridge.imgmsg_to_cv2(depth_image_message, "32FC1")
        except CvBridgeError as e:
            print(e)

        # Set the dimensions of the image
        self.dims = rgb_image.shape
        h = self.dims[0]
        w = self.dims[1]
        
        # Tranform image to gayscale
        #gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)

        # Do histogram equlization
        #img = cv2.equalizeHist(gray)

        # Detect the faces in the image
        #face_rectangles = self.face_detector(rgb_image, 0)
        blob = cv2.dnn.blobFromImage(cv2.resize(rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.face_net.setInput(blob)
        face_detections = self.face_net.forward()
        x_diff = 0
        face_distance = 0
        face_detected = False
        for i in range(0, face_detections.shape[2]):
            confidence = face_detections[0, 0, i, 2]
            if confidence>0.50:
                face_detected = True
                # rospy.loginfo("Face detected. WE ARE GOING TO CENTER IT")
                box = face_detections[0,0,i,3:7] * np.array([w,h,w,h])
                box = box.astype('int')
                x1, y1, x2, y2 = box[0], box[1], box[2], box[3]
                # center of the face 
                x_center = (x1+x2)/2.
                y_center = (y1+y2)/2.
                print('x_center', x_center)
                
                # how much is the centre of the face from the center of the image
                print('self dims 1 ',self.dims[1])
                x_diff = 600 / 2 - x_center
                # Find the distance to the detected face
                face_distance = float(np.nanmean(depth_image[y1:y2,x1:x2]))
                break
                # print('Distance to face', face_distance)

        if not face_detected:
            print("HUGE MISTAKE WE DID NOT DETECT THE FACE")
        # rotate the robot so that the face will be in the center of the camera view
        # if the face is not in the center of the camera view
        # if the face is in the center of the camera view then the robot will aproach the face
        # if the face is too close to the robot then the robot will stop
        # if the face is too far from the robot then the robot will stop
        # if x_diff > 0.1:

        #  i calculatet the FOV of the camera and its about 60degs

        # x_diff is here in pixels and the max value is 300 or -300
        # x_diff = x_diff / 300 # translate to [-1,1]
        # x_diff = x_diff / (1/6) # image is 1/6 of the whole space
            # rotate the robot to the left
            # we need to rotate the robot by x_diff * 60 to get number of degrees for rotation
            # since we have not 360 but from -1 to 1 rotations we scale it
            # we see 60degs which is 2/6
            # 1/3
            # we rotate by x_diff * 1/3 to the right and by x_diff * 1/3 to the left
        print("x_diff", x_diff)
        rotation_z = (x_diff / 300) / 6
        print("rotation_z", rotation_z)
        # get current position of robot
        current_pose = self.get_current_pose()
        # get the z rotation of the robot
        current_rotation_z = current_pose.pose.pose.orientation.z 
        # calculate the new rotation
        new_rotation_z = current_rotation_z + rotation_z
        if new_rotation_z > 1:
            new_rotation_z = -2 + new_rotation_z
        elif new_rotation_z < -1:
            new_rotation_z = 2 + new_rotation_z
        # create a new pose
        new_pose = PoseStamped()
        new_pose.header.seq = 100
        new_pose.header.frame_id = "map"
        new_pose.pose.position.x = current_pose.pose.pose.position.x
        new_pose.pose.position.y = current_pose.pose.pose.position.y
        new_pose.pose.position.z = current_pose.pose.pose.position.z
        new_pose.pose.orientation.x = current_pose.pose.pose.orientation.x
        new_pose.pose.orientation.y = current_pose.pose.pose.orientation.y
        new_pose.pose.orientation.z = new_rotation_z
        new_pose.pose.orientation.w = current_pose.pose.pose.orientation.w
        # create a new goal
        publish = rospy.Publisher('/move_base_simple/goal', PoseStamped, queue_size=10)
        publish.publish(new_pose)

        ## now we just need to calculate how much we need to move forward


        print("PUBLISHIG MESSAGE", new_pose)

        # rospy.loginfo("FACE SHOULD BE IN THE CENTER NOW")            

        pass
    
    def get_current_pose(self)->PoseWithCovarianceStamped:
        """
        Get the current pose of the robot
        """
        try:
            pose = rospy.wait_for_message("/amcl_pose", PoseWithCovarianceStamped)
            return pose
        except Exception as e:
            print(e)
            return None

    def find_faces(self):
        # print('I got a new image!')

        # Get the next rgb and depth images that are posted from the camera
        try:
            rgb_image_message = rospy.wait_for_message("/camera/rgb/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        # get robot position in moment of taking a picture
        p = rospy.wait_for_message("/amcl_pose", PoseWithCovarianceStamped)

        try:
            depth_image_message = rospy.wait_for_message("/camera/depth/image_raw", Image)
        except Exception as e:
            print(e)
            return 0

        # Convert the images into a OpenCV (numpy) format

        try:
            rgb_image = self.bridge.imgmsg_to_cv2(rgb_image_message, "bgr8")
        except CvBridgeError as e:
            print(e)

        try:
            depth_image = self.bridge.imgmsg_to_cv2(depth_image_message, "32FC1")
        except CvBridgeError as e:
            print(e)

        # Set the dimensions of the image
        self.dims = rgb_image.shape
        h = self.dims[0]
        w = self.dims[1]


        # Tranform image to gayscale
        #gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)

        # Do histogram equlization
        #img = cv2.equalizeHist(gray)

        # Detect the faces in the image
        #face_rectangles = self.face_detector(rgb_image, 0)
        blob = cv2.dnn.blobFromImage(cv2.resize(rgb_image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))
        self.face_net.setInput(blob)
        
        # no false positives yet, will keep as it is
        face_detections = self.face_net.forward()
        
        p_position = p.pose.pose.position
        p_orientation = p.pose.pose.orientation
 
        for i in range(0, face_detections.shape[2]):
            confidence = face_detections[0, 0, i, 2]
            if confidence>0.50:
                box = face_detections[0,0,i,3:7] * np.array([w,h,w,h])
                box = box.astype('int')
                x1, y1, x2, y2 = box[0], box[1], box[2], box[3]

                # Extract region containing face
                face_region = rgb_image[y1:y2, x1:x2]
                # cv2.imwrite('face.jpg', face_region) # Save the face image

                # Visualize the extracted face
                #cv2.imshow("ImWindow", face_region)
                #cv2.waitKey(1)

                # Find the distance to the detected face
                face_distance = float(np.nanmean(depth_image[y1:y2,x1:x2]))

                # print('Distance to face', face_distance)

                # Get the time that the depth image was recieved
                depth_time = depth_image_message.header.stamp

                # Find the location of the detected face
                pose = self.get_pose((x1,x2,y1,y2), face_distance, depth_time)

                # print(f"pose: {pose}")

                if pose is not None:
                    
                    # Create a marker used for visualization
                    self.marker_num += 1
                    marker = Marker()
                    marker.header.stamp = rospy.Time(0)
                    marker.header.frame_id = 'map'
                    marker.pose = pose
                    marker.type = Marker.CUBE
                    marker.action = Marker.ADD
                    marker.frame_locked = False
                    marker.lifetime = rospy.Duration.from_sec(10)
                    marker.id = self.marker_num
                    marker.scale = Vector3(0.1, 0.1, 0.1)
                    marker.color = ColorRGBA(0, 1, 0, 1)
                    d = self.face_similarity(pose, face_region)
                    # TODO: check why distance is inf (probably fix face-detection)
                    # rospy.loginfo(f"Distance is {d}.")
                    # message = "Will add" if d > self.FACE_DISTANCE_TOLERANCE else "will not add"
                    # rospy.loginfo(message)
                    if d > self.FACE_DISTANCE_TOLERANCE:
                        self.bot_positions.append((p_position, p_orientation, face_distance))
                        position = (pose.position.x, pose.position.y, pose.position.z)

                        self.face_positions_and_images.append((position, face_region))

                        greeting_pose = self.get_greeting_pose((x1,x2,y1,y2), face_distance, depth_time, p)
                        greeting_position = (
                                greeting_pose.position.x,
                                greeting_pose.position.y,
                                greeting_pose.position.z,
                                greeting_pose.orientation.z,
                        )
                        self.face_path.append(greeting_position)

                        # once face is detected, cancel goal and set go_to_face to true
                        cancel_msg = GoalID()

                        # UNCOMENT TO GO TO FACES
                        self.cancel_goal_pub.publish(cancel_msg)
                        self.path_idx -= 1
                        self.go_to_face = True

                        relative_path_to_image = os.path.join(os.path.dirname(__file__), f"../images/face_{len(self.face_path)}.jpg")
                        cv2.imwrite(relative_path_to_image, face_region) # Save the face image

                        self.marker_array.markers.append(marker)
                        self.markers_pub.publish(self.marker_array)
    

    def depth_callback(self,data):

        try:
            depth_image = self.bridge.imgmsg_to_cv2(data, "32FC1")
        except CvBridgeError as e:
            print(e)

        # Do the necessairy conversion so we can visuzalize it in OpenCV
        
        image_1 = depth_image / np.nanmax(depth_image)
        image_1 = image_1*255
        
        image_viz = np.array(image_1, dtype=np.uint8)

        #cv2.imshow("Depth window", image_viz)
        #cv2.waitKey(1)

        #plt.imshow(depth_image)
        #plt.show()

def main():
       # TODO: add sub on topic /amcl_pose
        # this sub should get current position (once!)
        # and sort self.path accordingly
        face_finder = face_localizer()


        
        # rospy.loginfo(f"Publishing on {TOPIC}")
        # sub = rospy.Subscriber("/move_base/status", GoalStatusArray, callback=face_finder.check_status)


        # print(face_finder.path)
        rate = rospy.Rate(10)
        
        # BOT NEEDS TO SLEEP BEFORE TO FUNCTION!
        rospy.sleep(1)
        face_finder.publish_new_position()                

        # seq_id = 1
        while not rospy.is_shutdown():
            face_finder.find_faces()
            
            reached, status = face_finder.status_reached() 
            if reached:
                if status == 3 and face_finder.go_to_face:
                    speak_to_person()
                    face_finder.go_to_face = False
                face_finder.publish_new_position()

            if face_finder.path_idx == len(face_finder.path) and face_finder.face_path_idx == len(face_finder.face_path):
                rospy.loginfo("All goals reached")
                break
                # else:
                #     rospy.loginfo("All goals reached.")
                #     rospy.loginfo(face_finder.face_path)
                #     face_finder.publish_new_position()
                #     while not face_finder.status_reached():
                #         pass
                #     face_finder.publish_new_position()
                #     break


            rate.sleep()

        cv2.destroyAllWindows()


if __name__ == '__main__':
    main()
